<div align="center">
<h1>
  XVERSE-65B
</h1>
</div>

<p align="center">
        <a href="https://huggingface.co/xverse/XVERSE-65B">ğŸ¤— XVERSE-65B</a>&nbspï½œ
        <a href="https://modelscope.cn/organization/xverse" rel="nofollow"><img src="resources/modelscope.png" width="20px" style="max-width: 100%;"> ModelScope</a>&nbspï½œ&nbsp
        <a href="resources/wechat.png">ğŸ’¬ å¾®ä¿¡ç¤¾åŒº</a>
</p>

<h4 align="left">
    <p>
        <b>ä¸­æ–‡</b> |
        <a href="README_EN.md">English</a>
    <p>
</h4>

## æ›´æ–°ä¿¡æ¯
**[2023/11/24]** æ›´æ–°é¢„è®­ç»ƒæ•°æ®çš„ç›¸å…³ä¿¡æ¯ã€‚  
**[2023/11/06]** å‘å¸ƒ 65B å°ºå¯¸çš„ XVERSE-65B åº•åº§æ¨¡å‹ã€‚  

## æ¨¡å‹ä»‹ç»

**XVERSE-65B** æ˜¯ç”±æ·±åœ³å…ƒè±¡ç§‘æŠ€è‡ªä¸»ç ”å‘çš„æ”¯æŒå¤šè¯­è¨€çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Modelï¼‰ï¼Œå‚æ•°è§„æ¨¡ä¸º 650 äº¿ï¼Œæœ¬æ¬¡å¼€æºçš„æ¨¡å‹ä¸ºåº•åº§æ¨¡å‹ **XVERSE-65B**ï¼Œä¸»è¦ç‰¹ç‚¹å¦‚ä¸‹ï¼š

- **æ¨¡å‹ç»“æ„**ï¼šXVERSE-65B ä½¿ç”¨ä¸»æµ Decoder-only çš„æ ‡å‡† Transformer ç½‘ç»œç»“æ„ï¼Œæ”¯æŒ 16K çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆContext Lengthï¼‰ï¼Œèƒ½æ»¡è¶³æ›´é•¿çš„å¤šè½®å¯¹è¯ã€çŸ¥è¯†é—®ç­”ä¸æ‘˜è¦ç­‰éœ€æ±‚ï¼Œæ¨¡å‹åº”ç”¨åœºæ™¯æ›´å¹¿æ³›ã€‚
- **è®­ç»ƒæ•°æ®**ï¼šæ„å»ºäº† 2.6 ä¸‡äº¿ token çš„é«˜è´¨é‡ã€å¤šæ ·åŒ–çš„æ•°æ®å¯¹æ¨¡å‹è¿›è¡Œå……åˆ†è®­ç»ƒï¼ŒåŒ…å«ä¸­ã€è‹±ã€ä¿„ã€è¥¿ç­‰ 40 å¤šç§è¯­è¨€ï¼Œé€šè¿‡ç²¾ç»†åŒ–è®¾ç½®ä¸åŒç±»å‹æ•°æ®çš„é‡‡æ ·æ¯”ä¾‹ï¼Œä½¿å¾—ä¸­è‹±ä¸¤ç§è¯­è¨€è¡¨ç°ä¼˜å¼‚ï¼Œä¹Ÿèƒ½å…¼é¡¾å…¶ä»–è¯­è¨€æ•ˆæœã€‚
- **åˆ†è¯**ï¼šåŸºäº BPEï¼ˆByte-Pair Encodingï¼‰ç®—æ³•ï¼Œä½¿ç”¨ä¸Šç™¾ GB è¯­æ–™è®­ç»ƒäº†ä¸€ä¸ªè¯è¡¨å¤§å°ä¸º 100,534 çš„åˆ†è¯å™¨ï¼Œèƒ½å¤ŸåŒæ—¶æ”¯æŒå¤šè¯­è¨€ï¼Œè€Œæ— éœ€é¢å¤–æ‰©å±•è¯è¡¨ã€‚
- **è®­ç»ƒæ¡†æ¶**ï¼šè®­ç»ƒä¸­é‡‡ç”¨ FlashAttention2 åŠ é€Ÿè®¡ç®—ï¼Œ3D å¹¶è¡ŒåŸºç¡€ä¸Šé‡‡ç”¨è™šæ‹Ÿæµæ°´çº¿ï¼ˆvirtual pipelineï¼‰æŠ€æœ¯ï¼Œé™ä½è¾ƒé•¿æµæ°´çº¿å’Œ 16k ä¸Šä¸‹æ–‡çª—å£äº§ç”Ÿçš„è¿‡é«˜æ°”æ³¡ç‡ï¼Œåœ¨åƒå¡é›†ç¾¤çš„å³°å€¼ç®—åŠ›åˆ©ç”¨ç‡è¾¾åˆ°ä¸šç•Œå‰åˆ—ã€‚åŒæ—¶é€šè¿‡é›†ç¾¤åŸºç¡€è®¾æ–½è¿è¥ã€èµ„æºè°ƒåº¦ã€è®­ç»ƒæ¡†æ¶å’Œè°ƒåº¦å¹³å°ååŒç­‰æŒç»­ä¼˜åŒ–ï¼Œæ‰“é€ å‡ºé«˜ç¨³å®šã€ä½ä¸­æ–­ã€å¼ºå®¹é”™çš„è®­ç»ƒç³»ç»Ÿï¼Œå°†æ¯å‘¨æœ‰æ•ˆè®­ç»ƒç‡æå‡è‡³ 98.6%ã€‚

åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œ**XVERSE-65B** ä¸»è¦ä½¿ç”¨äº† 7 ç±»ä¸åŒçš„æ•°æ®ç±»å‹ã€‚ä»¥ä¸‹è¡¨æ ¼å±•ç¤ºäº† XVERSE-65B ä¸å…¶ä»–ä¸€äº›çŸ¥åæ¨¡å‹åœ¨é¢„è®­ç»ƒæ•°æ®é›†æ–¹é¢çš„æ¯”è¾ƒï¼š

| æ•°æ®ç±»åˆ« | GPT3[^1] | Llama[^2] | BLOOM[^3] | PaLM[^4] | Chinchilla[^5] | Gopher[^6] | MT-NLG[^7] | XVERSE-65B |
|:-------:|:--------:|:---------:|:---------:|:--------:|:--------------:|:----------:|:----------:|:----------:|
| ç½‘é¡µç±»   | Y        | Y         | Y         | Y        | Y              | Y          | Y          | Y          |
| ä»£ç ç±»   |          | Y         | Y         | Y        | Y              | Y          | Y          | Y          |
| ç™¾ç§‘ç±»   | Y        | Y         |           | Y        | Y              | Y          | Y          | Y          |
| ä¹¦ç±ç±»   | Y        | Y         |           | Y        | Y              | Y          | Y          | Y          |
| è®ºæ–‡ç±»   |          | Y         |           |          |                |            | Y          | Y          |
| é—®ç­”ç±»   | Y        | Y         |           | Y        |                |            | Y          | Y          |

> æ³¨ï¼š'Y' è¡¨ç¤ºä½¿ç”¨äº†è¯¥ç±»æ•°æ®ã€‚

åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œä¸åŒç±»åˆ«æ•°æ®çš„é‡‡æ ·æ¯”ä¾‹å¦‚ä¸‹æ‰€ç¤ºï¼š
|         | ç½‘é¡µç±» | ä»£ç ç±» | ç™¾ç§‘ç±» | ä¹¦ç±ç±» | è®ºæ–‡ç±» | é—®ç­”ç±» | å…¶ä»–ç±» |
|:-------:|:------:|:------:|:------:|:------:|:------:|:------:|:------:|
| æ¯”ä¾‹(%) | 72.91  | 7.09   | 4.81   | 5.62   | 6.55   | 1.15   | 1.87   |

[^1]: GPT3 Paper: [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
[^2]: LLaMA Paper: [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)
[^3]: BLOOM Paper: [BLOOM: A 176B-Parameter Open-Access Multilingual Language Model](https://arxiv.org/abs/2211.05100)
[^4]: PaLM Paper: [PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/abs/2204.02311)
[^5]: Chinchilla Paper: [Training Compute-Optimal Large Language Models](https://arxiv.org/pdf/2203.15556)
[^6]: Gopher Paper: [Scaling Language Models: Methods, Analysis & Insights from Training Gopher](https://arxiv.org/abs/2112.11446)
[^7]: MT-NLG Paper: [Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model](https://arxiv.org/abs/2201.11990)

## è¯„æµ‹ç»“æœ

ä¸ºäº†ç»¼åˆè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ï¼Œæˆ‘ä»¬åœ¨ä¸€ç³»åˆ—æ ‡å‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†å…¨é¢æµ‹è¯•ï¼ŒåŒ…æ‹¬C-Evalã€CMMLUã€Gaokao-Benchã€MMLUã€GAOKAO-Englishã€AGIEvalã€RACE-Mã€CommonSenseQAã€PIQAã€GSM8Kå’ŒHumanEvalã€‚è¿™äº›è¯„ä¼°è¦†ç›–äº†æ¨¡å‹åœ¨å¤šä¸ªé¢†åŸŸçš„èƒ½åŠ›ï¼Œå…·ä½“åŒ…æ‹¬ä¸­æ–‡é—®ç­”ã€è‹±æ–‡é—®ç­”ã€è¯­è¨€ç†è§£ã€å¸¸è¯†é—®ç­”ã€é€»è¾‘æ¨ç†ã€æ•°å­¦é—®é¢˜è§£ç­”ä»¥åŠç¼–ç¨‹èƒ½åŠ›ã€‚è¯„ä¼°ç»“æœå¦‚ä¸‹ï¼š

|  èƒ½åŠ›ç»´åº¦  |           æ•°æ®é›†           |        | XVERSE-65B | Llama1-65B | Llama2-70B | Falcon-180B | GPT-3.5 | GPT-4 |
| :--------: | :------------------------: | :----: | :--------: | :--------: | :--------: | :---------: | :-----: | :---: |
|  ä¸­æ–‡é—®ç­”  |           C-Eval           | 5-shot |    68.6    |    38.8    |    49.9    |    54.2     |  54.4   | 68.7  |
|            |           CMMLU            | 5-shot |    72.6    |    40.6    |    53.6    |    57.2     |  53.9   | 71.0  |
|            |  Gaokao-Bench<sup>1</sup>  | 5-shot |    73.9    |    38.9    |    51.4    |    50.5     |    -    |   -   |
|  è‹±æ–‡é—®ç­”  |            MMLU            | 5-shot |    70.8    |    63.4    |    68.9    |    70.5     |  70.0   | 86.4  |
|            | GAOKAO-English<sup>1</sup> | 5-shot |    85.3    |    67.0    |    76.6    |    63.3     |    -    |   -   |
| ä¸­è‹±æ–‡é—®ç­” |    AGIEval<sup>1</sup>     | 5-shot |    61.8    |    42.4    |    51.4    |    51.3     |    -    |   -   |
|  è¯­è¨€ç†è§£  |           RACE-M           | 0-shot |    90.6    |    67.9    |    81.5    |    87.6     |  85.6   | 93.7  |
|  å¸¸è¯†é—®ç­”  |       CommonSenseQA        | 7-shot |    79.8    |    74.0    |    78.5    |    82.4     |  80.2   | 88.3  |
|    æ¨ç†    |            PIQA            | 0-shot |    80.4    |    82.8    |    82.8    |    85.3     |  81.7   | 89.2  |
|    æ•°å­¦    |           GSM8K            | 4-shot |    60.3    |    50.9    |    56.8    |    62.6     |  57.1   | 92.0  |
|    ä»£ç     |         HumanEval          | 0-shot |    26.8    |    23.7    |    29.9    |      -      |  48.1   | 67.0  |

> <sup>1ï¼šåªé’ˆå¯¹å…¶ä¸­çš„å•é¡¹é€‰æ‹©é¢˜è¿›è¡Œæµ‹è¯•ï¼Œå³æ’é™¤äº†å¡«ç©ºé¢˜ã€å¼€æ”¾æ€§é—®é¢˜å’Œå¤šé¡¹é€‰æ‹©é¢˜</sup>   

å¯¹äºä¸Šè¿°æ‰€æœ‰æ¯”è¾ƒæ¨¡å‹ï¼Œæˆ‘ä»¬ä¼˜å…ˆæ±‡æŠ¥å…¶å®˜æ–¹å…¬å¸ƒçš„ç»“æœã€‚åœ¨ç¼ºå°‘å®˜æ–¹ç»“æœçš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬é‡‡ç”¨äº† [OpenCompass æ¦œå•](https://opencompass.org.cn/leaderboard-llm)çš„æŠ¥å‘Šç»“æœã€‚å…¶ä»–ç»“æœåˆ™æ¥è‡ªäºæˆ‘ä»¬è‡ªè¡Œæ‰§è¡Œçš„è¯„ä¼°æµç¨‹æ‰€è·å¾—çš„æ•°æ®ã€‚   
å¯¹äº MMLU ï¼Œæˆ‘ä»¬é‡‡ç”¨ä½œè€…æä¾›çš„[è¯„æµ‹å·¥å…·](https://github.com/hendrycks/test)ï¼ŒC-Evalã€AGIEvalã€GAOKAO-Benchã€GAOKAO-English ä¸ MMLU çš„è¯„æµ‹æ–¹å¼ç›¸åŒï¼Œå…¶ä½™è¯„æµ‹æ•°æ®é›†ä½¿ç”¨ [OpenCompass è¯„ä¼°æ¡†æ¶](https://github.com/open-compass/OpenCompass/)è¿›è¡Œè¯„ä¼°ã€‚

## ä½¿ç”¨æ–¹æ³•

### ç¡¬ä»¶éœ€æ±‚
ä¸‹è¡¨åˆ—å‡ºäº†åœ¨ XVERSE-65B ä¸Šè¿›è¡Œæ¨ç†å’Œå¾®è°ƒæ‰€éœ€è¦çš„ç¡¬ä»¶èµ„æºï¼š
|            | ç±»å‹ | æ–¹æ³•             | å†…å­˜   | GPU        |
| ---------- | ---- | ---------------- | ------ | ---------- |
| XVERSE-65B | è®­ç»ƒ | LoRA with ZeRO-3 | 1500GB | 8*A800 80G |
| XVERSE-65B | æ¨ç† | BF16/FP16        | 500GB  | 2*A800 80G |

### ç¯å¢ƒå®‰è£…

1. ä¸‹è½½æœ¬ä»“åº“ï¼š

```shell
git clone https://github.com/xverse-ai/XVERSE-65B
cd XVERSE-65B
```

2. ä½¿ç”¨ pip å®‰è£…ä¾èµ–ï¼š

```shell
pip install -r requirements.txt
```
### Transformers åŠ è½½æ–¹å¼

å¯é€šè¿‡ä»¥ä¸‹ä»£ç åŠ è½½ XVERSE-65B æ¨¡å‹æ¥è¿›è¡Œæ¨ç†ï¼š

```python
>>> import torch
>>> from transformers import AutoTokenizer, AutoModelForCausalLM
>>> tokenizer = AutoTokenizer.from_pretrained("xverse/XVERSE-65B")
>>> model = AutoModelForCausalLM.from_pretrained("xverse/XVERSE-65B", trust_remote_code=True, torch_dtype=torch.bfloat16, device_map='auto')
>>> model = model.eval()
>>> inputs = tokenizer('åŒ—äº¬çš„æ™¯ç‚¹ï¼šæ•…å®«ã€å¤©å›ã€ä¸‡é‡Œé•¿åŸç­‰ã€‚\næ·±åœ³çš„æ™¯ç‚¹ï¼š', return_tensors='pt').input_ids
>>> inputs = inputs.cuda()
>>> generated_ids = model.generate(inputs, max_new_tokens=64, eos_token_id=tokenizer.eos_token_id, repetition_penalty=1.1)
>>> print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True))
```

### ç½‘é¡µ Demo

å¯é€šè¿‡ä»¥ä¸‹ä»£ç å¯åŠ¨ä¸€ä¸ªweb serverï¼Œåœ¨æµè§ˆå™¨è¾“å…¥è®¿é—®åœ°å€åï¼Œå¯ä½¿ç”¨ XVERSE-65B æ¨¡å‹è¿›è¡Œæ¨ç†ï¼š

```shell
python text_generation_demo.py --port='port' --model_path='/path/to/model/' --tokenizer_path='/path/to/tokenizer/'
```

### æ¨¡å‹å¾®è°ƒ

XVERSE-65B æ”¯æŒå¼€å‘è€…è¿›è¡Œå¾®è°ƒä»¥å®ç°æ›´å¥½çš„æ€§èƒ½è¡¨ç°ã€‚åœ¨æ­¤æˆ‘ä»¬å°è¯•ä½¿ç”¨ [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) ä¸ XVERSE-65B è¿›è¡Œå…¼å®¹æ€§å¾®è°ƒè®­ç»ƒï¼Œå¹¶åœ¨ 8 * Nvidia A800 80 GB + DeepSpeed çš„ç¯å¢ƒä¸‹è¿›è¡Œäº†æµ‹è¯•ã€‚
ä¸‹é¢æˆ‘ä»¬ç»™å‡ºäº†ä½¿ç”¨`LoRA with ZeRO-3`çš„å¾®è°ƒæ–¹æ³•ã€‚

#### ç¯å¢ƒå‡†å¤‡

ä¸‹è½½ LLaMA-Factory é¡¹ç›®å¹¶æŒ‰å…¶è¦æ±‚[å®‰è£…ä¾èµ–](https://github.com/hiyouga/LLaMA-Factory#getting-started)ã€‚

#### å¯åŠ¨è®­ç»ƒ

è®­ç»ƒå¯åŠ¨è„šæœ¬ï¼š
> å…¶ä¸­ model_path è¯·æ›¿æ¢ä¸ºè‡ªå·±çš„æ¨¡å‹è·¯å¾„

> XVERSE-65B åŸºäº bfloat16 è®­ç»ƒçš„ï¼Œå»ºè®®é€‰ç”¨ bfloat16 åšå¾®è°ƒè®­ç»ƒã€‚
```bash
deepspeed --num_gpus 8 src/train_bash.py \
    --deepspeed deepspeed.json \
    --stage sft \
    --model_name_or_path model_path  \
    --do_train \
    --dataset alpaca_gpt4_zh \
    --template default \
    --finetuning_type lora \
    --lora_target q_proj,v_proj \
    --output_dir  output_model_path \
    --overwrite_cache \
    --per_device_train_batch_size 4 \
    --gradient_accumulation_steps 4 \
    --lr_scheduler_type cosine \
    --logging_steps 1 \
    --save_steps 1000 \
    --learning_rate 5e-5 \
    --num_train_epochs 3.0 \
    --plot_loss \
    --bf16
```
deep_speed.json å‚æ•°é…ç½®ï¼š
```json
{
    "train_micro_batch_size_per_gpu":"auto",
    "gradient_accumulation_steps":"auto",
    "gradient_clipping":"auto",
    "zero_allow_untested_optimizer":true,
    "fp16":{
        "enabled":false
    },
    "bfloat16":{
        "enabled":true
    },
    "zero_optimization":{
        "stage":3,
        "allgather_partitions":true,
        "reduce_scatter":true,
        "overlap_comm":false,
        "contiguous_gradients":true
    }
}
```

## å±€é™æ€§ä¸å…è´£ç”³æ˜

XVERSE-65B ä¸å…¶ä»–æ‰€æœ‰ LLM ä¸€æ ·ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹å¯èƒ½ä¼šäº§ç”Ÿä¸å‡†ç¡®ã€æœ‰åè§æˆ–å…¶ä»–ä»¤äººåæ„Ÿçš„å†…å®¹ã€‚å› æ­¤ï¼Œè¯·è°¨æ…ä½¿ç”¨æ¨¡å‹ç”Ÿæˆçš„å†…å®¹ï¼Œè¯·å‹¿å°†ç”Ÿæˆçš„æœ‰å®³å†…å®¹è¿›è¡Œä¼ æ’­ï¼Œåœ¨éƒ¨ç½²ä»»ä½• XVERSE-65B çš„åº”ç”¨ä¹‹å‰ï¼Œå¼€å‘äººå‘˜åº”æ ¹æ®å…¶å…·ä½“åº”ç”¨å¯¹æ¨¡å‹è¿›è¡Œå®‰å…¨æµ‹è¯•å’Œè°ƒä¼˜ã€‚

æˆ‘ä»¬å¼ºçƒˆè­¦å‘Šä¸è¦å°† XVERSE-65B æ¨¡å‹ç”¨äºåˆ¶é€ æˆ–ä¼ æ’­æœ‰å®³ä¿¡æ¯ï¼Œæˆ–è¿›è¡Œä»»ä½•å¯èƒ½æŸå®³å…¬ä¼—ã€å›½å®¶ã€ç¤¾ä¼šå®‰å…¨æˆ–è¿åæ³•è§„çš„æ´»åŠ¨ã€‚å¦‚æœä½¿ç”¨ XVERSE-65B æ¨¡å‹äº§ç”Ÿä»»ä½•é—®é¢˜ï¼Œæ— è®ºæ˜¯æ•°æ®å®‰å…¨é—®é¢˜ã€å…¬å…±èˆ†è®ºé£é™©ï¼Œè¿˜æ˜¯æ¨¡å‹è¢«è¯¯è§£ã€æ»¥ç”¨ã€ä¼ æ’­æˆ–ä¸åˆè§„ä½¿ç”¨æ‰€å¼•å‘çš„ä»»ä½•é£é™©å’Œé—®é¢˜ï¼Œæˆ‘ä»¬å°†ä¸æ‰¿æ‹…ä»»ä½•è´£ä»»ã€‚

## æ¨¡å‹å¼€æºåè®®

ä½¿ç”¨æœ¬ä»“åº“çš„æºç éœ€è¦éµå¾ª [Apache-2.0](LICENSE) å¼€æºåè®®ï¼Œä½¿ç”¨ XVERSE-65B çš„æ¨¡å‹æƒé‡åˆ™éœ€è¦éµå¾ª[æ¨¡å‹è®¸å¯åè®®](MODEL_LICENSE.pdf)ã€‚

XVERSE-65B æ¨¡å‹æƒé‡å¯¹å­¦æœ¯ç ”ç©¶**å®Œå…¨å¼€æ”¾**ï¼Œå¹¶ä¸”æ”¯æŒ**å…è´¹å•†ç”¨**ã€‚å¦‚éœ€ç”³è¯·å•†ä¸šè®¸å¯è¯ï¼Œè¯·å¡«å†™ã€[ç”³è¯·è¡¨](https://chat.xverse.cn/home/business.html)ã€‘ï¼Œå¦‚æœ‰å…¶ä»–é—®é¢˜æˆ–åˆä½œï¼Œè¯·è”ç³» <opensource@xverse.cn>ã€‚

